<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic - Editorial by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="/assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<!-- Header -->
								<header id="header">
									<ul class="icons" style="font-size: 1.5em;">
										<li><a href="https://github.com/MateiCosa" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
										<li><a href="https://www.linkedin.com/in/mateicosa/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
										<li><a href="mailto: mateigabriel.cosa@studbocconi.it" class="icon fa-envelope"><span class="label"></span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>AI Takes Over Cannes: <br> My experience at WAICF 2023</h1>
									</header>

									<span class="image main"><img src="/images/cannes_1.jpeg" alt="" /></span>

									<p>In February 2023, I had the chache to visit Cannes, the place famous for its beaches, hotels, casino, celebrities, and of course, its International Film Festival. But I was there a for a different kind of festival, the <b>World AI Cannes Festival (WAICF)</b>. This event brought together firms, universities, tech enthuziasts, laypersons, and some of the most prominent leaders in the industry. With countless exhibitors, start-ups, workshops, and conferences, WAICF 2023 had all the ingredients to be a memorable experience.</p>
									<p>Fortunately, this was an experience that I shared with friends from the <b>Bocconi AI & Neuroscience Student Association (BAINSA)</b>, a group of talented and abitious people with a passion for AI. Our trip was made possible by our sponsor, <b>Bending Spoons</b>, as well as our partner, <b>Istituto EuropIA</b>, which was also the organizer of the festival. This allowed us to attend the event not only as curious visitors, but also as volunteers helping the conference staff and the exhibitors.</p>
									
									<span class="image main"><img src="/images/cannes_2.jpeg" alt="" /></span>

									<hr class="major" />

									<span class="image right"><img src="/images/cannes_3.jpeg" alt="" /></span>

									<h2>Yann LeCun on limitations of LLM's & AGI</h2>
									<p>One of the hotspots of the festival was Yann LeCun's talk on the state of AI. With the world buzzing about <b>ChatGPT</b> and the potential of <b>Large Language Models (LLM's)</b>, LeCun's presentation focused on explaining the beginnings, as the well as the latest developments of this exciting sub-field of AI. He recognized the utility of these models, which have alredy started shaking up not just tech world, but many other industries. In LeCun's view, LLM's are great tools and companions for productivity tasks and a variety of use cases.</p>
									<p>On the other hand, LeCun warned against viewing LLM's as a form <b>"Artificial General Intelligence" (AGI)</b>, which refers to a form of AI capable of performing virtually anything that a human can do (and more). Altough there may be sparks of AGI in models like ChatGPT, LeCun does not see the current paradigm as a path to attaining this ultimate goal of the AI research community. This is because, regardless of the immense amounts of data that one feeds to algorithm or the overwhelming model complexity, such architectures remain vulnerable to the most basic questions,like elementary algebra or other forms or reasoning which come naturally to humans. Lacking the "common sense" of a human being, these models would fail to notice blatantly absurd statements, thereby confidently outputting non-sense. </p>
									<p>The solution to this problem is, in LeCun's view, coming up with a <b>"World Model"</b>. Such a model would be a representation of the common sense that we use without even thinking about it. An example of this is knowning that the apple from the tree falls to the ground instead of floating or that it will not snow during the summer (let's hope climate change will not become this bad!). The idea of distinguishing the probable from the improbable, or the possible form the absurd is something humans use all the time to reason about the world. Leveraging this type of common sense might unlock new ways of improving the way we think about AI. A teenager can learn to drive a car in about 20 hours, but autonomous driving systems need billions of labeled data and a lot of computing power. This is why Yann LeCun and his team <b>Meta</b> are working on designing world models to better explain the world.</p>

									<span class="image right"><img src="/images/cannes_4.jpeg" alt="" /></span>

									<hr class="major" />

									<h2>Transformers and beyond</h2>
									<p>Another interesting experience was attending a panel discussion on <b>Transformer</b> models and <b>Deep Learning</b> in general. One of the speakers was in fact one of the authors of the original paper "Attention Is All You Need" that introduced the Transformer architecture: Aidan Gomez. He was joined by Thomas Wolf, co-founder of the amazing <b>Hugging Face</b> platform and Maria Zuluaga, professor of machine learning at <b>King's College London</b>. The speakers discussed the birth of the Transformer architecture and its impressive rise to the forefront of modern AI. Gomez admitted that he and his colleagues did not forsee the huge impact of their research until the paper was published. The quick adoption of this technology by academia and industry alike were an indicator of their success. </p>
									<p>The Transfomer model revolutionized <b>Natural Language Processing (NLP)</b> by taking advantage of the self-attention mechanism that allows the model to weigh the importance of words/tokens in order to better deal with long-term dependencies. This architecture gave rise to the LLM's like ChatGPT that are so prevalent nowadays. With this in mind, the speakers were faced with a difficult question: will there be something else to replace the Transformer? Since 2017, this go-to architecture showed unmatched performance, with little variation from its original form. In fact, the debate has shifted from a question of the right model to a <b>question of the underlying data</b>. Training a billion parameter model requires an immense amount of data, not to mention the necessary computational power. This is where the brilliant people at Hugging Face come in with their remarkable pre-trained models that are easy to use and ussualy perform well out-of-the-box. Until a new architecture comes along to dethrone the Transformer, fine-tuning LLM's for specific use cases seems like the way to go.</p>


									<hr class="major" />

									<h2>The future looks bright</h2>
									<p>My four-day visit to Cannes left me optimistic and excited about the future of AI. The variety of tools and applications on display at WAICF was an inspiring sight. From business intelligence and chatbots, to medical and quantum applications, it seems that we are living through a "golden age" of AI. Even my morning coffee came from an AI-powered machine, which was, of course, a completly unnecesary use of this technology.</p>
									<p>With great potential, however, come great risks. It was reassuring to see that explainable AI and ethics were big talking points at the festival. From disinformation, to automated decision making in critical sectors, frameworks for protecting society against potentially dangerous uses of artificial intelligence are needed. With the right goals in mind, I believe AI will be a tool to shape humanity for the better!</p>
									<span class="image main"><img src="/images/cannes_5.jpeg" alt="" /></span>
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="/index.html">Home</a></li>
										<li><a href="/pages/blog.html">Blog</a></li>
										<li><a href="/pages/projects.html">Projects</a></li>
										<li><a href="/#about">About</a></li>
									</ul>
								</nav>

							<!-- Section -->
							<section>
								<header class="major">
									<h2>Get in touch</h2>
								</header>
								<p>Interested in a project collaboration, looking for a passionate and hard-working intern or simply wish to share some interesting information? Feel free to contact me at:</p>
								<ul class="contact">
									<li class="icon solid fa-envelope"><a href="mailto:mateigabriel.cosa@studbocconi.it">mateigabriel.cosa@studbocconi.it</a></li>
								</ul>
							</section>

						<!-- Footer -->
							<footer id="footer">
								<p class="copyright">&copy; Matei Gabriel Cosa. <br> Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
							</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="/assets/js/jquery.min.js"></script>
			<script src="/assets/js/browser.min.js"></script>
			<script src="/assets/js/breakpoints.min.js"></script>
			<script src="/assets/js/util.js"></script>
			<script src="/assets/js/main.js"></script>

	</body>
</html>